---
title: "2_nuseds_cuid_pse"
author: "Bruno S. Carturan, Eric Hertz, Stephanie J. Peacock"
date: "2025-02-27"
output: 
  html_document:
    toc: true                  # Adds a table of contents to the document
    toc_float: true           # Makes the table of contents float on the side as the reader scrolls.
    toc_collapsed: true        # Starts the table of contents in a collapsed state.
    toc_depth: 3               # Specifies the depth of headers (e.g., ##, ###) to include in the table of contents.
    number_sections: true      # 
    theme: journal  # lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

The goal of the script is to associate each population (defined by the fields `POP_ID` = `IndexId`, the latter also specifying the species acronym) in the **cleaned NuSEDS** dataset (i.e., *NuSEDS_escapement_data_collated_DATE.csv*) to the conservation unit identidication number `cuid`, as defined in the Pacific Salmon explorer ([PSE](https://www.salmonexplorer.ca/)). The cleaning procedure is coded in the script *1_nuseds_collation.rmd* and is visible in [1_nuseds_collation.html](https://bookdown.org/salmonwatersheds/nuseds_cleaning_procedure/1_nuseds_collation.html).


```{r,include=FALSE}

#'******************************************************************************
#' The goal of the script is to 
#' 
#' Previous script: Fraser_salmon_CU_updates.Rmd
#' 
#' 
#' Files imported (from dropbox):
#' - streamlocationids.csv
#' - conservationunits_decoder.csv
#' - streamspawnersurveys_output.csv
#' - 1_NuSEDS_escapement_data_collated_DATE.csv
#'
#' Files produced: 
#' - 2_nuseds_cuid_streamid_DATE.csv
#' 
#'******************************************************************************

# rm(list = ls())
graphics.off()

# reset the wd to head using the location of the current script
path <- rstudioapi::getActiveDocumentContext()$path
dirhead <- "population-indicators"
path_ahead <- sub(pattern = paste0("\\",dirhead,".*"),replacement = "", x = path)
wd_head <- paste0(path_ahead,dirhead)
setwd(wd_head)

# Now import functions related to directories.
# Note that the script cannot be called again once the directory is set to the 
# subdirectory of the project (unless setwd() is called again).
source("code/functions_set_wd.R")
source("code/functions_general.R")

# return the name of the directories for the different projects:
subDir_projects <- subDir_projects_fun()

wds_l <- set_working_directories_fun(subDir = subDir_projects$spawner_surveys,
                                     Export_locally = F)
wd_head <- wds_l$wd_head
wd_project <- wds_l$wd_project
wd_code <- wds_l$wd_code
wd_data <- wds_l$wd_data
wd_figures <- wds_l$wd_figures
wd_output <- wds_l$wd_output
wd_X_Drive1_PROJECTS <- wds_l$wd_X_Drive1_PROJECTS

wd_references_dropbox <- paste(wd_X_Drive1_PROJECTS,
                               wds_l$wd_project_dropbox,
                               "references",sep="/")

wd_data_dropbox <- paste(wd_X_Drive1_PROJECTS,
                         wds_l$wd_project_dropbox,
                         "data",sep="/")

wd_documents <- paste(wd_project,"documents",sep="/")

wd_pop_indic_data_input_dropbox <- paste(wd_X_Drive1_PROJECTS,
                                         wds_l$wd_population_indicator_data_input_dropbox,
                                         sep = "/")

wd_pop_indic_data_gis_dropbox <- gsub("input","gis",wd_pop_indic_data_input_dropbox)

# Loading packages & functions
library(tidyr)
library(dplyr) # for arrange()
library(sf)
library(sp)     # for spDists() TERRA is the replacement
library(scales)

source("code/functions.R")


#'* !!! DECISIONS TO MAKE !!! *

# Set tp true to update the datasets
export_datasets <- F

#' Decision made in 1_nuseds_collation.rmd: 
#' Remove the IndexId & GFE_ID time series in NUSEDS with only NAs and/or 0s
#' If TRUE, time series only composed of NAs and/or 0s are removed from NUSEDS; 
#' If FALSE, time series only composed of NAs are removed from NUSEDS.
#' Setting to T or F will import the corresponding dataset.
remove_timeSeries_zeros_too <- T 

#' Decision made in 1_nuseds_collation.rmd: Used just after NUSEDS and CUSS are merged,
#'to decide if we want to replace all the remaining 0s by NAs.
#' Setting to T or F will import the corresponding dataset.
replace_zeros_byNAs <- T

```

# Import datasets

Import the cleaned NuSEDS dataset with the following decisions made concerning zeros:

```{r, include=FALSE}

#'* Import the log file *
logfile_path <- paste0(getwd(),"/spawner-surveys/output/1_log_file.csv")
logfile_path <- gsub("spawner-surveys/code/","",logfile_path)
log_file <- read.csv(logfile_path,header = T)

# selected the rows corresponding to the decision made concerning including or 
# not the 0s:
cond <- log_file$remove_timeSeries_zeros_too == remove_timeSeries_zeros_too & 
  log_file$replace_zeros_byNAs == replace_zeros_byNAs
# select the most recent date:
date_export <- max(log_file[cond,]$date_export)


#'* Import the most recent NuSEDS_escapement_data_collated file *
# nuseds <- import_mostRecent_file_fun(wd = paste0(wd_output,"/archive"), 
#                                      pattern = "NuSEDS_escapement_data_collated")

nuseds <- read.csv(paste0(wd_output,"/archive/1_NuSEDS_escapement_data_collated_",date_export,".csv"),header = T)

head(nuseds)
nrow(nuseds) # 310255 309338 306823 306999

nrow(unique(nuseds[,c("SPECIES_QUALIFIED","POP_ID","SYSTEM_SITE","WATERBODY")])) # 6892 6854 6868
nrow(unique(nuseds[,c("SPECIES_QUALIFIED","POP_ID","SYSTEM_SITE","WATERBODY","GFE_ID")])) # 6892 6854 6868

sum(nuseds$MAX_ESTIMATE == 0 & !is.na(nuseds$MAX_ESTIMATE)) # 0
sum(is.na(nuseds$MAX_ESTIMATE)) # 157808 157264 155984
```

```{r,echo=FALSE}

if(remove_timeSeries_zeros_too){
  print("Time series only composed of NAs AND/OR zeros were removed from NUSEDS in the early cleaning process in 1_nuseds_collation.rmd.")
}else{
  print("Time series only composed of NAs were removed from NUSEDS in the early cleaning process in 1_nuseds_collation.rmd, zeros were left.")
}

if(replace_zeros_byNAs){
  print("Zeros were replaced by NAs in the late cleaning process in 1_nuseds_collation.rmd.")
}else{
  print("Zeros were NOT replaced by NAs in the late cleaning process in 1_nuseds_collation.rmd.")
}

```
The PSE file *conservationunits_decoder.csv* is imported; this is the file containing the current `cuid` associated to NuSEDS's `FULL_CU_IN` (named `cu_index` in the decoder file).

```{r,include=FALSE}

#'Import the name of the different datasets in the PSF database and their 
#' corresponding CSV files.
datasetsNames_database <- datasetsNames_database_fun()

fromDatabase <- update_file_csv <- F

#'* Import the conservationunits_decoder.csv *
#' from population-indicators/data_input or download it from the PSF database.
#' # To obtain the generation length and calculate the the "current spawner abundance".
conservationunits_decoder <- datasets_database_fun(nameDataSet = datasetsNames_database$name_CSV[1],
                                                   fromDatabase = fromDatabase,
                                                   update_file_csv = update_file_csv,
                                                   wd = wd_pop_indic_data_input_dropbox)

head(conservationunits_decoder)

# check the CUs with is.na(cu_index):
sum(is.na(conservationunits_decoder$cu_index))    # 45
sum(is.na(conservationunits_decoder$cu_name_pse))
cond <- is.na(conservationunits_decoder$cu_index)
conservationunits_decoder[cond,]
```

Import the *streamlocationids.csv* to access the location names, coordinates and identification number for maintaining consistency.

**TEMPORARY**: change the name of the field `streamid` to `stream_cu_id` because the latter is the unique association between a CU (`cuid`) and a location/stream (`GFE_ID`).

```{r, include=FALSE}
#'* Files from PSF database *

#'Import the name of the different datasets in the PSF database and their 
#' corresponding CSV files.
datasetsNames_database <- datasetsNames_database_fun()

fromDatabase <- update_file_csv <- F

#' Import streamlocationids to obtain the streamID 
streamlocationids <- datasets_database_fun(nameDataSet = "streamlocationids.csv", # datasetsNames_database$name_CSV[9],
                                           fromDatabase = fromDatabase,
                                           update_file_csv = update_file_csv,
                                           wd = wd_pop_indic_data_input_dropbox)

streamlocationids$sys_nm <- gsub("\\\\","'",streamlocationids$sys_nm)
streamlocationids$sys_nm <- gsub("  "," ",streamlocationids$sys_nm)

rownames(streamlocationids) <- NULL
colnames(streamlocationids)[colnames(streamlocationids) == "streamid"] <- "stream_cu_id"

# add the field region from the decoder
streamlocationids$region <- NA
for(rg_i in unique(streamlocationids$regionid)){
  cond <- conservationunits_decoder$drv_regionid == rg_i
  region <- unique(conservationunits_decoder$region[cond])
  
  cond <- streamlocationids$regionid == rg_i
  streamlocationids$region[cond] <- region
}

# remove unnecessary and confusing fields
streamlocationids <- streamlocationids[,c("region","sys_nm","GFE_ID","latitude","longitude","cu_name_pse","cuid","stream_cu_id")]
rownames(streamlocationids) <- NULL

head(streamlocationids)
```

Import the regions' shape files are imported. These represent the geographic boundaries as displayed in the PSE.

```{r, include=FALSE}
#'* Import the shape files for the Region boundaries  *
# wd_maps_rg <- gsub("1_PROJECTS","5_DATA",wd_X_Drive1_PROJECTS) # files not up to date
# wd_maps_rg <- gsub("1_PROJECTS","5_DATA",wd_X_Drive1_PROJECTS)
# wd_maps_rg <- paste0(wd_pop_indic_data_gis_dropbox,"/se_boundary_regions")
regions_shp <- st_read(paste0(wd_pop_indic_data_gis_dropbox,"/se_boundary_regions/se_boundary_regions.shp")) %>%
  st_transform(crs = 4269)
unique(regions_shp$regionname)
sf_use_s2(FALSE) # so that st_intersects() and st_simplify() can be used
regions_shp_full <- regions_shp
regions_shp <- st_simplify(x = regions_shp, dTolerance = .002) # .001 # to reduce computation time

```


# Updates on FULL_CU_IN/cu_index

## In conservationunits_decoder (TEMPORARY)

The updates below are done here until the changes are made directly in *conservationunits_decoder.csv*.

```{r, echo=FALSE}

d_fixes <- data.frame(cuid = c(528,756,758,759,760,761,763,1022),
                      cu_index_old = c("SX_528","","","","","","","SER-023"),
                      cu_index_new = c("SEL-16-01","SEL-03-07","SEL-05-01","SEL-06-19",
                                       "SEL-09-04","SEL-09-05","SEL-10-02","SER-23"))

for(r in 1:nrow(d_fixes)){
  cuid <- d_fixes$cuid[r]
  cu_index_new <- d_fixes$cu_index_new[r]
  cond <- conservationunits_decoder$cuid == cuid
  conservationunits_decoder$cu_index[cond] <- cu_index_new
}

d_fixes
```

## In NuSEDS

The `FULL_CU_IN` of several population is updated to reflect (1) partition of the Sockeye CU `FULL_CU_IN` = "SEL-21-02" into sub-groups (EW: 'Early Wild', for Babine/Onerka; LW: 'Late Wild' for Nilkitkwa, and F: Fulton and Pinkut) as in [DFO 2023](https://waves-vagues.dfo-mpo.gc.ca/library-bibliotheque/41102356.pdf); (2) recent corrections for the Bella Coola River-Late CU (CM-17) and Bella Coola-Dean Rivers (CM-16) (personal communication from Carrie Holt, DFO, May 2023). The field `FULL_CU_IN_PSE` is created to reflect these changes.

```{r, echo=FALSE}
#'* Edit FULL_CU_IN for several POP_IDs * 
# Corrections in CU assignment for central coast chum from Carrie Holt
# https://salmonwatersheds.slack.com/archives/C017N5NSCJY/p1683774240661029?thread_ts=1683735939.696999&cid=C017N5NSCJY
# https://salmonwatersheds.slack.com/archives/CJ5RVHVCG/p1705426563165399?thread_ts=1705344122.088409&cid=CJ5RVHVCG

nuseds$FULL_CU_IN_PSE <- nuseds$FULL_CU_IN

#' Import the corrections:
full_cu_l <- update_for_FULL_CU_IN_l()

show <- NULL
for(i in 1:length(full_cu_l)){
  # i <- 5
  FULL_CU_IN_here <- names(full_cu_l)[i]
  # print(FULL_CU_IN_here)
  
  POP_IDs_here <- full_cu_l[[i]]
  
  #
  cond_nuseds <- nuseds$POP_ID %in% POP_IDs_here
  
  if(any(cond_nuseds)){
    show_here <- unique(nuseds[cond_nuseds,c("POP_ID","FULL_CU_IN")])
  
    cond_decoder <- conservationunits_decoder$cu_index == FULL_CU_IN_here &
      !is.na(conservationunits_decoder$cu_index)
  
    if(!any(cond_decoder)){
      print("not match for FULL_CU_IN_here in decoder - BREAL")
      break
    
    }else{
      show_here$region <- unique(conservationunits_decoder$region[cond_decoder])
      show_here$species_name <- unique(conservationunits_decoder$species_name[cond_decoder])
      show_here$FULL_CU_IN_PSE <- FULL_CU_IN_here
      show_here <- show_here[,c(3,4,1,2,5)]
      rownames(show_here) <- NULL
    
      if(is.null(show)){
        show <- show_here
      }else{
        show <- rbind(show,show_here)
      }
      
      nuseds$FULL_CU_IN_PSE[cond_nuseds] <- FULL_CU_IN_here
    }
  }
}

show
```

Additionally, the `CU_NAME` of the river Sockeye at `SYSTEM` = "BELLA COOLA RIVER" (`GFE_ID` = 968) is changed from:

```{r, echo=FALSE}
#'* FIX: South Atnarko Lakes *
#' GFE_ID 968 for sockeye should be attributed to South Atnarko Lakes CU 
#' (cf. Population meeting from 05/03/2024)
cond <- nuseds$GFE_ID == 968 & nuseds$SPECIES_QUALIFIED %in% c("SEL","SER")
# unique(nuseds$CU_NAME[cond]) # "NORTHERN COASTAL FJORDS"
# unique(nuseds$SPECIES_QUALIFIED[cond]) # SER
# unique(nuseds$SYSTEM_SITE[cond]) # "BELLA COOLA RIVER"

show <- unique(nuseds[cond,c("SPECIES_QUALIFIED","CU_NAME","SYSTEM_SITE","GFE_ID")])
rownames(show) <- NULL
show
```

to:

```{r, echo=FALSE}
nuseds$CU_NAME[cond] <- toupper("South Atnarko Lakes")
show <- unique(nuseds[cond,c("SPECIES_QUALIFIED","CU_NAME","SYSTEM_SITE","GFE_ID")])
rownames(show) <- NULL
show
```

# Edits on Methods

## Create stream_survey_quality & survey_score

We create the field `stream_survey_quality` and survey_score from `ESTIMATE_CLASSIFICATION` such as:

```{r, echo=FALSE}

#'* Create stream_survey_quality from ESTIMATE_CLASSIFICATION *
#' cf. Table 4.5 in section 4.1.3 of the Tech Report
estim_class_nuseds <- unique(nuseds$ESTIMATE_CLASSIFICATION)

nuseds$stream_survey_quality <- NA
for(ecn in estim_class_nuseds){
  # ecn <- estim_class_nuseds[1]
  cond_nuseds <- nuseds$ESTIMATE_CLASSIFICATION == ecn
  
  if(ecn == "TRUE ABUNDANCE (TYPE-1)"){
    out <- "High"
  }else if(ecn == "TRUE ABUNDANCE (TYPE-2)"){
    out <- "Medium-High"
  }else if(ecn == "RELATIVE ABUNDANCE (TYPE-3)"){
    out <- "Medium"
  }else if(ecn == "RELATIVE ABUNDANCE (TYPE-4)"){
    out <- "Medium-Low"
  }else if(ecn %in% c("RELATIVE ABUNDANCE (TYPE-5)",
                      "RELATIVE: CONSTANT MULTI-YEAR METHODS")){
    out <- "Low"
  }else if(ecn %in% c("PRESENCE/ABSENCE (TYPE-6)",
                      "PRESENCE-ABSENCE (TYPE-6)",
                      "RELATIVE: VARYING MULTI-YEAR METHODS")){
    out <- "Low"
  }else if(ecn == "UNKNOWN"){
    out <- "Unknown"
  }else if(ecn %in% c("","NO SURVEY THIS YEAR","NO SURVEY")){
    out <- NA
  }else{
    print(ecn)
  }
  #print(out)
  nuseds$stream_survey_quality[cond_nuseds] <- out
}

# survey_score:
# cf. Table 4.5 in section 4.1.3 of the Tech Report
# nuseds <- import_mostRecent_file_fun(wd = paste0(wd_output,"/archive"),
#                                      pattern = "nuseds_cuid_streamid_")
estim_class_nuseds <- unique(nuseds$ESTIMATE_CLASSIFICATION)
estim_class_nuseds

nuseds$survey_score <- NA
for(ecn in estim_class_nuseds){
  # ecn <- estim_class_nuseds[1]
  cond_nuseds <- nuseds$ESTIMATE_CLASSIFICATION == ecn
  
  if(ecn == "TRUE ABUNDANCE (TYPE-1)"){
    out <- "1"
  }else if(ecn == "TRUE ABUNDANCE (TYPE-2)"){
    out <- "2"
  }else if(ecn == "RELATIVE ABUNDANCE (TYPE-3)"){
    out <- "3"
  }else if(ecn == "RELATIVE ABUNDANCE (TYPE-4)"){
    out <- "4"
  }else if(ecn %in% c("RELATIVE ABUNDANCE (TYPE-5)",
                      "RELATIVE: CONSTANT MULTI-YEAR METHODS")){
    out <- "5"
  }else if(ecn %in% c("PRESENCE/ABSENCE (TYPE-6)",
                      "PRESENCE-ABSENCE (TYPE-6)",
                      "RELATIVE: VARYING MULTI-YEAR METHODS")){
    out <- "6"
  }else if(ecn == "UNKNOWN"){
    out <- "Unknown"
  }else if(ecn %in% c("","NO SURVEY THIS YEAR","NO SURVEY")){
    out <- NA
  }else{
    print(ecn)
  }
  #print(out)
  nuseds$survey_score[cond_nuseds] <- out
}

nuseds$survey_score |> unique()

show <- unique(nuseds[,c("ESTIMATE_CLASSIFICATION","stream_survey_quality","survey_score")])
rownames(show) <- NULL

ESTIMATE_CLASSIFICATION <- c("TRUE ABUNDANCE (TYPE-1)",
                             "TRUE ABUNDANCE (TYPE-2)",
                             "RELATIVE ABUNDANCE (TYPE-3)",
                             "RELATIVE ABUNDANCE (TYPE-4)",
                             "RELATIVE ABUNDANCE (TYPE-5)",
                             "PRESENCE-ABSENCE (TYPE-6)",
                             "RELATIVE: CONSTANT MULTI-YEAR METHODS",
                             "RELATIVE: VARYING MULTI-YEAR METHODS",
                             "UNKNOWN",
                             "NO SURVEY THIS YEAR")

order <- order(factor(x = show$ESTIMATE_CLASSIFICATION, levels = ESTIMATE_CLASSIFICATION))
show <- show[order,]
show
```

## Fixes in ESTIMATE_METHOD 

We make the following corrections to the field `ESTIMATE_METHOD`:

```{r, echo=FALSE}

#'* Fixes in the methods *
# Katy's request:
# https://salmonwatersheds.slack.com/archives/C03LB7KM6JK/p1712611492405689?thread_ts=1712252256.802999&cid=C03LB7KM6JK

# unique(nuseds$ESTIMATE_METHOD)

estMeth <- data.frame(ESTIMATE_METHOD = c("Cummulative","Unknown","Fixed Site Census",
                                          "Aerial","Fence", "Insufficient Information"),
                      correction = c("Cumulative","Unknown Estimate Method","Fence Count",
                                     "Aerial Survey","Fence Count","Unknown Estimate Method"))


estMeth

for(r in 1:nrow(estMeth)){
  # r <- 2
  if(estMeth$ESTIMATE_METHOD[r] == "Cummulative"){
    nuseds$ESTIMATE_METHOD <- gsub("Cummulative","Cumulative",nuseds$ESTIMATE_METHOD)
  }else{
    cond <- nuseds$ESTIMATE_METHOD == estMeth$ESTIMATE_METHOD[r]
    nuseds$ESTIMATE_METHOD[cond] <- estMeth$correction[r]
  }
}

```

## Create survey_score 

# Add the survey_score field -------

```{r, echo=FALSE}

```




# Find the cuid from FULL_CU_IN_PSE

We associate the previously defined field `FULL_CU_IN_PSE` to the PSE's `cuid`, `cu_name_pse`, `cu_name_dfo`, `region`, using the *conservationunits_decoder.csv*. For instance:

```{r, include=FALSE}

#'* Provide a cuid to each row in nuseds using conservationunits_decoder *
#' using: FULL_CU_IN_PSE

nuseds$cuid <- NA
nuseds$cu_name_pse <- NA
nuseds$cu_name_dfo <- NA
nuseds$region <- NA
nuseds$regionid <- NA

FULL_CU_IN_PSE <- unique(nuseds$FULL_CU_IN_PSE)
# length(FULL_CU_IN_PSE) # 411
# sum(is.na(FULL_CU_IN_PSE)) # 0

cuid_cu_index <- data.frame(FULL_CU_IN_PSE = FULL_CU_IN_PSE)
cuid_cu_index$cuid <- NA
cuid_cu_index$CU_NAME <- NA
for(r in 1:nrow(cuid_cu_index)){
  fci <- cuid_cu_index$FULL_CU_IN_PSE[r]
  cond <- conservationunits_decoder$cu_index == fci & !is.na(conservationunits_decoder$cu_index)
  cond2 <- nuseds$FULL_CU_IN_PSE == fci
  CU_NAME <- paste(unique(nuseds$CU_NAME[cond2]), collapse = "; ") # the upates done above creates three instance where more than one CU_NAME is returned for one unique FULL_CU_IN_PSE in nuseds
  cuid_cu_index$CU_NAME[r] <- CU_NAME
  
  # if(length(unique(nuseds$CU_NAME[cond2])) > 1){ # check ; but these are due to the changes made above with update_for_FULL_CU_IN_l()
  #   print(cuid_cu_index[r,])
  #   print(unique(nuseds$CU_NAME[cond2]))
  #   print("***")
  # }
  
  if(any(cond)){
    cuid_cu_index$cuid[r] <- unique(conservationunits_decoder$cuid[cond])
    nuseds$cuid[cond2] <- unique(conservationunits_decoder$cuid[cond])
    nuseds$cu_name_pse[cond2] <- unique(conservationunits_decoder$cu_name_pse[cond])
    nuseds$cu_name_dfo[cond2] <- unique(conservationunits_decoder$cu_name_dfo[cond])
    nuseds$region[cond2] <- unique(conservationunits_decoder$region[cond])
  }
}
```

```{r, echo=FALSE}
show <- unique(nuseds[,c("region","SPECIES","CU_NAME","cu_name_dfo","cu_name_pse","FULL_CU_IN_PSE","cuid")])

rownames(show) <- NULL
head(show)
```

```{r, include=FALSE}
#'* Check the FULL_CU_IN not in the decoder: *
cond_NA <- is.na(cuid_cu_index$cuid)
head(cuid_cu_index)
sum(cond_NA) # 23

cu_index_NA <- cuid_cu_index[cond_NA,]
colnames(cu_index_NA)[colnames(cu_index_NA) == "FULL_CU_IN_PSE"] <- "FULL_CU_IN"

rownames(cu_index_NA) <- NULL
```

There are `r nrow(cu_index_NA)` `FULL_CU_IN` that are not in *conservationunits_decoder.csv*. We use the regions' shape file to find their respective region:

```{r,echo=FALSE}

# Find the coordinates
cu_index_NA$CU_LAT <- sapply(cu_index_NA$FULL_CU_IN,function(cui){
  cond <- nuseds$FULL_CU_IN == cui
  return(unique(nuseds$CU_LAT[cond]))
})

cu_index_NA$CU_LONGT <- sapply(cu_index_NA$FULL_CU_IN,function(cui){
  cond <- nuseds$FULL_CU_IN == cui
  return(unique(nuseds$CU_LONGT[cond]))
})

#' Find the corresponding region:
cu_index_NA$region <- NA
for(r in 1:nrow(cu_index_NA)){
  # r <- 1
  point <- st_as_sf(cu_index_NA[r,], 
                    coords = c("CU_LONGT","CU_LAT"), crs = 4269)
  
  layer_rg <- st_intersects(point, regions_shp)
  if(length(layer_rg[[1]]) == 0){ # no match, try to buffer
    layer_rg <- st_intersects(point, st_buffer(x = regions_shp, dist = .001))
  }
  
  if(length(layer_rg[[1]]) == 0){ # no match, try to buffer
    layer_rg <- st_intersects(point, st_buffer(x = regions_shp, dist = .002))
  }
  
  if(length(layer_rg[[1]]) == 0){ # no match, try to buffer
    print("Still no region found - BREAK")
    break
  }
  
  if(length(layer_rg[[1]]) > 1){ # 
    print("More than one region found - BREAK")
    break
  }
  
  layer_rg <- layer_rg[[1]]
  cu_index_NA$region[r] <- regions_shp$regionname[layer_rg]
}

cu_index_NA
```

```{r, include=FALSE}
# used to be cuid 751
cond_751 <- grepl(simplify_string_fun("Adams and Momich Lakes_Early Summer"),
              simplify_string_fun(nuseds$CU_NAME))
nuseds[cond_751,]$WATERBODY |> unique()
nuseds[cond_751,]$CU_NAME |> unique()
nuseds[cond_751,]$FULL_CU_IN |> unique()

nuseds[cond_751 , c("cuid","WATERBODY","Y_LAT","X_LONGT")] |> unique()
#  cuid            WATERBODY latitude_final longitude_final
#   751         BURTON CREEK       51.48255       -119.4648 --> 760  Adams-Early Summer
#   751 MOMICH RIVER - UPPER       51.31958       -119.3236 --> 761 Momich-Early Summer
#   751  ADAMS RIVER - UPPER       51.41090       -119.4561 --> 760  Adams-Early Summer
#   751         MOMICH RIVER       51.33461       -119.4232 --> 761 Momich-Early Summer
#   751        CAYENNE CREEK       51.32071       -119.3197 --> 761 Momich-Early Summer
```

The CU above with `FULL_CU_IN` = `r unique(nuseds[cond_751,]$FULL_CU_IN)` is separated into the two following CUs to reflect COSEWIC's grouping (note the creation of the field `cu_name_pse`):


```{r, echo=FALSE}
# UPDATE: 2024-11-21: the CU Fraser Sockeye Adams & Momich Lakes-Early Summer  
# 751 was split into the CUs with cuid 760 and 761. Consequently cuid 751 is not
# in the decoder anymore. For the sake of updating the data for the data demise
# paper, I add it here:
# This section should be removed in the next NuSEDS update.
# CU Fraser Sockeye Adams & Momich Lakes-Early Summer  751 that was split into CUs 760 and 761
# https://salmonwatersheds.slack.com/archives/C03LB7KM6JK/p1725753313593719?thread_ts=1725564850.867719&cid=C03LB7KM6JK
# cf. population meeting September 11 2024

cond_760 <- cond_751 & nuseds$WATERBODY %in% c("BURTON CREEK","ADAMS RIVER - UPPER")
cond_761 <- cond_751 & nuseds$WATERBODY %in% c("MOMICH RIVER - UPPER","MOMICH RIVER",
                                               "CAYENNE CREEK")

nuseds$cuid[cond_760] <- 760
nuseds$cuid[cond_761] <- 761

for(cuid in 760:761){
  cond_nuseds <- nuseds$cuid == cuid & !is.na(nuseds$cuid)
  cond_decoder <- conservationunits_decoder$cuid == cuid
  nuseds$cu_name_pse[cond_nuseds] <- conservationunits_decoder$cu_name_pse[cond_decoder]
  nuseds$cu_name_dfo[cond_nuseds] <- conservationunits_decoder$cu_name_dfo[cond_decoder]
  nuseds$FULL_CU_IN_PSE[cond_nuseds] <- conservationunits_decoder$cu_index[cond_decoder]
  nuseds$region[cond_nuseds] <- conservationunits_decoder$region[cond_decoder]
}

show <- nuseds[cond_751,c("FULL_CU_IN","FULL_CU_IN_PSE","CU_NAME","cu_name_pse","cuid","WATERBODY")] |> unique()
rownames(show) <- NULL
show[order(show$cuid),]

# cond_751 <- grepl(simplify_string_fun("Adams and Momich Lakes_Early Summer"),
#               simplify_string_fun(nuseds$CU_NAME))
# nuseds[cond_751,]$cuid <- 751
# nuseds[cond_751,]$cu_name_pse <- "Adams & Momich Lakes-Early Summer"
# nuseds[cond_751,]$cu_name_dfo <- "Adams and Momich Lakes_Early Summer <<Extinct>>"
# nuseds[cond_751,]$regionid <- 4
# nuseds[cond_751,]$region <- "Fraser"

```

We show below the corresponding time series for the rest of the unmatched CUs:

```{r, echo=FALSE, fig.height=7.5, fig.width=10}
# update cu_index_NA
cond <- grepl(simplify_string_fun("Adams and Momich Lakes_Early Summer"),
              simplify_string_fun(cu_index_NA$CU_NAME))
cu_index_NA <- cu_index_NA[!cond,]

# produce figure time series
for(r in 1:nrow(cu_index_NA)){
  # r <- 1
  FULL_CU_IN <- cu_index_NA$FULL_CU_IN[r]
  cond <- nuseds$FULL_CU_IN == FULL_CU_IN
  IndexId_GFE_ID <- unique(nuseds[cond,c("IndexId","GFE_ID")])
  
  main <- paste(cu_index_NA$CU_NAME[r],
                cu_index_NA$FULL_CU_IN[r],
                sep = " - ")
  
  plot_IndexId_GFE_ID_fun(IndexIds = IndexId_GFE_ID$IndexId,
                          GFE_IDs = IndexId_GFE_ID$GFE_ID,
                          all_areas_nuseds = nuseds, 
                          main = c(cu_index_NA$region[r],main))
  legend("topright",paste0("r = ",r), bty = "n")
}
```

These time series are kept in the dataset but will be removed in the next step/script.


# Work on locations


## Corrections

There are two `LOCAL_NAME_1` with typos, which we replace by the value in `SYSTEM_SITE`:

```{r, echo=FALSE}
# Manual fix 1st:
#'-  "BARRI\xc8RE RIVER" --> "BARRIERE RIVER"
#'- "FRAN\xc7OIS LAKE" --> FRANCOIS LAKE"

cond1 <- nuseds$SYSTEM_SITE == "BARRIERE RIVER"
cond2 <- nuseds$SYSTEM_SITE == "FRANCOIS LAKE"

show <- unique(nuseds[cond1 | cond2,c("SYSTEM_SITE","WATERBODY","LOCAL_NAME_1","LOCAL_NAME_2")])
rownames(show) <- NULL

nuseds$LOCAL_NAME_1[cond1] <- "BARRIERE RIVER"
nuseds$LOCAL_NAME_1[cond2] <- "FRANCOIS LAKE"

show
```

We also create the field `sys_nm`, which is equivalent of `SYSTEM_SITE` but the following corrections or modification for the following locations:

```{r, include = FALSE}

# Most changes are in streamlocationids

# some changes are also in this list:
SYSTEM_SITE_fixes <- SYSTEM_SITE_fixes_fun()

SYSTEM_SITE_fixes <- data.frame(SYSTEM_SITE = SYSTEM_SITE_fixes$SYSTEM_SITE,
                                sys_nm = SYSTEM_SITE_fixes$sys_nm)

SYSTEM_SITE_fixes$sys_nm <- gsub('\\\\',"",SYSTEM_SITE_fixes$sys_nm)

# Only retain the SYSTEM_SITE in nuseds (all of them should be but just in case)
cond <- SYSTEM_SITE_fixes$SYSTEM_SITE %in% nuseds$SYSTEM_SITE
SYSTEM_SITE_fixes <- SYSTEM_SITE_fixes[cond,]

# Select all the locations in nuseds
nuseds_streams <- unique(nuseds[,c("SYSTEM_SITE","GFE_ID")])
# nrow(nuseds_streams) # 2325

# Add the corresponding streamlocationids$sys_nm
nuseds_streams$sys_nm <- sapply(nuseds_streams$GFE_ID, function(gfeid){
  cond <- streamlocationids$GFE_ID == gfeid & !is.na(streamlocationids$GFE_ID)
  if(!any(cond)){
    out <- NA
  }else{
    out <- unique(streamlocationids$sys_nm[cond])
  }
  
  if(length(out) > 1){
    out <-  paste0(out, collapse = "; ")
    out <- paste0("MORE THAN ONE LOCATION: ",out)
  }
  
  return(out)
})

# Retain the cases where there is a difference in the name:
cond <- !is.na(nuseds_streams$sys_nm) &
  simplify_string_fun(nuseds_streams$sys_nm) != simplify_string_fun(nuseds_streams$SYSTEM_SITE)
change1 <- nuseds_streams[cond,]

# Sites in SYSTEM_SITE_fixes not in streamlocationids
cond <- SYSTEM_SITE_fixes$sys_nm %in% streamlocationids$sys_nm
sum(!cond)
SYSTEM_SITE_fixes[!cond,]

# retain those in nuseds
cond2 <- SYSTEM_SITE_fixes[!cond,]$SYSTEM_SITE %in% nuseds$SYSTEM_SITE
change2 <-  SYSTEM_SITE_fixes[!cond,][cond2,]

# find GFE_ID
change2$GFE_ID <- sapply(change2$SYSTEM_SITE,function(ss){
  cond <- nuseds$SYSTEM_SITE == ss
  return(unique(nuseds$GFE_ID[cond]))
})
change2 <- change2[,c("GFE_ID","SYSTEM_SITE","sys_nm")]

# check if there is any change2$SYSTEM_SITE in change1$SYSTEM_SITE
cond <- change2$SYSTEM_SITE %in% change1$SYSTEM_SITE
change2[cond,] # if no all good

# change2$sys_nm_2 <-  sapply(change2$SYSTEM_SITE,function(ss){
#   cond <- change1$SYSTEM_SITE == ss
#   out <- NA
#   if(any(cond)){
#     out <- change1$sys_nm[cond]
#   }
#   return(out)
# })

# Combine the two
cols <- c("GFE_ID","SYSTEM_SITE","sys_nm")
change_all <- rbind(change1[,cols],change2[,cols])
change_all <- change_all[order(change_all$SYSTEM_SITE),]
rownames(change_all) <- NULL
```

```{r, echo=FALSE}
change_all[, c("SYSTEM_SITE","sys_nm","GFE_ID")]
```


```{r, include=FALSE}

# add the field the nuseds
nuseds$sys_nm <- nuseds$SYSTEM_SITE
for(r in 1:nrow(change_all)){
  cond <- nuseds$GFE_ID == change_all$GFE_ID[r]
  
  if(any(cond)){ # should not need this security
    nuseds$sys_nm[cond] <- change_all$sys_nm[r]
  }
}
```

##  Attribute stream_cu_id to nuseds

The field `stream_cu_id` is a unique combination between a stream (`GFE_ID`) and CU (`cuid`). We use the PSE file *streamlocationids* to match the existing `stream_cu_id`, for instance: 

```{r, echo=FALSE}
head(streamlocationids)
```

```{r include=FALSE}
nuseds$stream_cu_id <- NA

for(r in 1:nrow(streamlocationids)){
  # r <- 1
  cond <- !is.na(streamlocationids$GFE_ID[r]) &
    nuseds$GFE_ID == streamlocationids$GFE_ID[r] &
    !is.na(nuseds$cuid) & 
     nuseds$cuid == streamlocationids$cuid[r]
  
  if(any(cond)){
    nuseds$stream_cu_id[cond] <- streamlocationids$stream_cu_id[r]
  }
}

check <- unique(nuseds[,c("GFE_ID","sys_nm","cuid","cu_name_pse","stream_cu_id")])
nrow(check) # 6880

cond <- is.na(check$stream_cu_id) & !is.na(check$cuid)
show <- check[cond,]
rownames(show) <- NULL
```

There remains `r sum(is.na(check$stream_cu_id))` unique stream (`GFE_ID`) and CU (`cuid`) combinations without an existing `stream_cu_id` (not accounting for the time series in **nuseds** without a `cuid`):

```{r, echo=FALSE}
show
```

We give them a new `stream_cu_id` value by simply incrementing from the maximum current `stream_cu_id` value.

```{r, include=FALSE}
val <- max(streamlocationids$stream_cu_id)
for(r in 1:nrow(show)){
  val <- val + 1
  show$stream_cu_id[r] <- val
  
  cond <- nuseds$GFE_ID == show$GFE_ID[r] &
    nuseds$cuid == show$cuid[r] & !is.na(nuseds$cuid)
  
  nuseds$stream_cu_id[cond] <- show$stream_cu_id[r] 
}
```



```{r, include=FALSE}
# Katy's request to check 
# https://salmonwatersheds.slack.com/archives/CJG0SHWCW/p1741726887009909?thread_ts=1741129100.647379&cid=CJG0SHWCW

stream_cu_ids_check <- c(640,381,2500,2741,2653,2742,2725,2785,4014,4260,930,727)

cond <- streamlocationids$stream_cu_id %in% stream_cu_ids_check
streamlocationids[cond,]

GFE_ID_to_check <- unique(streamlocationids[cond,]$GFE_ID)
GFE_ID_to_check 
length(GFE_ID_to_check) # 8

streampse <- unique(streamlocationids[cond,c("region","sys_nm","GFE_ID","latitude","longitude")]) |> 
  arrange(region,sys_nm)

r_odd <- 1:nrow(streampse)
r_odd <- r_odd[r_odd %% 2 == 1]
for(r in r_odd){
  # r <- r_odd[1]
  GFE_ID_here <- streampse$GFE_ID[c(r,r + 1)]
  
  d_here <- data.frame(dataset = "streamlocationids",
                       region = streampse$region[c(r,r + 1)],
                       location_name = streampse$sys_nm[c(r,r + 1)],
                       GFE_ID =  streampse$GFE_ID[c(r,r + 1)],
                       latitude = streampse$latitude[c(r,r + 1)],
                       longitude = streampse$longitude[c(r,r + 1)])
  
   # nuseds
   cond1 <- nuseds$GFE_ID %in% GFE_ID_here
   d <- unique(nuseds[cond1,c("region","sys_nm","GFE_ID","Y_LAT","X_LONGT")])
   d$dataset <- "nuseds"
   d <- d[,c("dataset","region","sys_nm","GFE_ID","Y_LAT","X_LONGT")]
   colnames(d) <- c("dataset","region","location_name","GFE_ID","latitude","longitude")
   d_here <- rbind(d_here,d)
   
   # DFO_All_Streams_Segments
   cond1 <- DFO_All_Streams_Segments$ID %in% GFE_ID_here
   d <- unique(DFO_All_Streams_Segments[cond1,c("NME","ID","Y_LAT","X_LONGT")])
   d$dataset <- "DFO_All_Streams_Segments"
   d$region <- NA
   d <- d[,c("dataset","region","NME","ID","Y_LAT","X_LONGT")]
   colnames(d) <- c("dataset","region","location_name","GFE_ID","latitude","longitude")
   d_here <- rbind(d_here,d)
   
   # 
   for(gfeid in GFE_ID_here){
     # gfeid <- GFE_ID_here[1]
     
     # check in STOCK_GFE_ID
     cond1 <- DFO_hatchery$STOCK_GFE_ID %in% gfeid
     cond2 <- DFO_hatchery$REL_GFE_ID %in% gfeid
     if(any(cond1)){
       
       d <- unique(DFO_hatchery[cond1,c("STOCK_GFE_NAME","STOCK_GFE_ID","STOCK_LATITUDE","STOCK_LONGITUDE")])
       d$dataset <- "DFO_hatchery"
       d$region <- NA
       d <- d[,c("dataset","region","STOCK_GFE_NAME","STOCK_GFE_ID","STOCK_LATITUDE","STOCK_LONGITUDE")]
       colnames(d) <- c("dataset","region","location_name","GFE_ID","latitude","longitude")
       
     }else if(any(cond2)){
       
       d <- unique(DFO_hatchery[cond2,c("REL_WATERBODY_NAME","REL_GFE_ID","REL_LATITUDE","REL_LONGITUDE")])
       d$dataset <- "DFO_hatchery"
       d$region <- NA
       d <- d[,c("dataset","region","REL_WATERBODY_NAME","REL_GFE_ID","REL_LATITUDE","REL_LONGITUDE")] # RELEASE_SITE_NAME
       colnames(d) <- c("dataset","region","location_name","GFE_ID","latitude","longitude")
       
     }else{
       
       d <- data.frame(dataset = "DFO_hatchery",location_name = NA, GFE_ID = gfeid)
       d$region <- d$latitude <- d$longitude <- NA
       
     }
     
     d_here <- rbind(d_here,d)
   }

   d_here$latitude <- round(as.numeric(d_here$latitude),6)
   d_here$longitude <- round(as.numeric(d_here$longitude),6)
   
   d_here <- d_here[order(d_here$GFE_ID),]
   
   rownames(d_here) <- NULL
   
   print(d_here)
   print("")
}

```

# Export datasets

The following files are exported:

- *2_nuseds_cuid_streamid__DATE.csv*: the cleaned NuSEDS dataset with the PSE's field: `cuid`, `stream_cu_id`, `cu_name_pse`, etc.

- *log_file.csv*: the log file reporting the name of the main file exported, the date of the export, the name of the present script, the name of the original **NUSEDS** or **CUSS** files and the choices related to removing zeros or not.

```{r, include=FALSE}
if(export_datasets){
  
  date <- as.character(Sys.Date())

  name_file <- paste0("2_nuseds_cuid_streamid_",date)
  write.csv(nuseds,paste0(wd_output,"/archive/",name_file,".csv"),
            row.names = F)

  # Edit the log file locally (needs to be pushed to github)
  logfile_path <- paste0(getwd(),"/spawner-surveys/output/log_file.csv")
  logfile_path <- gsub("spawner-surveys/code/","",logfile_path)
  log_file <- read.csv(logfile_path,header = T)
  
  cond_date_export <- log_file$date_export == date_export
  
  log_file_new <- log_file[1,]
  log_file_new$main_dataset_exported <- name_file
  log_file_new$R_script <- "2_nuseds_cuid_pse.rmd"
  log_file_new$date_export <- date
  log_file_new$all_areas_nuseds_file_name <- log_file$all_areas_nuseds_file_name[cond_date_export]
  log_file_new$conservation_unit_system_sites_file_name <- log_file$conservation_unit_system_sites_file_name[cond_date_export]
  log_file_new$remove_timeSeries_zeros_too <- log_file$remove_timeSeries_zeros_too[cond_date_export]
  log_file_new$replace_zeros_byNAs <- log_file$replace_zeros_byNAs[cond_date_export]
  
  log_file <- rbind(log_file,log_file_new)
  
  wd_output_local <- getwd()
  wd_output_local <- gsub("code","output",wd_output_local)
  
  write.csv(log_file,paste0(wd_output_local,"/log_file.csv"),row.names = F)
  
}

```
























