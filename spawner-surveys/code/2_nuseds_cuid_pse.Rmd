---
title: "2_nuseds_cuid_pse"
author: "Bruno S. Carturan, Eric Hertz, Stephanie J. Peacock"
date: "2025-02-27"
output: 
  html_document:
    toc: true                  # Adds a table of contents to the document
    toc_float: true           # Makes the table of contents float on the side as the reader scrolls.
    toc_collapsed: true        # Starts the table of contents in a collapsed state.
    toc_depth: 3               # Specifies the depth of headers (e.g., ##, ###) to include in the table of contents.
    number_sections: true      # 
    theme: journal  # lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

The goal of the script is to associate each population (defined by the fields `POP_ID` = `IndexId`, the latter also specifying the species acronym) in the **cleaned NuSEDS** dataset (i.e., *NuSEDS_escapement_data_collated_DATE.csv*) to the conservation unit identidication number `cuid`, as defined in the Pacific Salmon explorer ([PSE](https://www.salmonexplorer.ca/)). The cleaning procedure is coded in the script *1_nuseds_collation.rmd* and is visible in [1_nuseds_collation.html](https://bookdown.org/salmonwatersheds/nuseds_cleaning_procedure/1_nuseds_collation.html).


```{r,include=FALSE}

#'******************************************************************************
#' The goal of the script is to 
#' 
#' Previous script: Fraser_salmon_CU_updates.Rmd
#' 
#' 
#' Files imported (from dropbox):
#' - streamlocationids.csv
#' - conservationunits_decoder.csv
#' - streamspawnersurveys_output.csv
#' - 1_NuSEDS_escapement_data_collated_DATE.csv
#'
#' Files produced: 
#' - 2_nuseds_cuid_streamid_DATE.csv
#' 
#'******************************************************************************

rm(list = ls())
graphics.off()

# reset the wd to head using the location of the current script
path <- rstudioapi::getActiveDocumentContext()$path
dirhead <- "population-indicators"
path_ahead <- sub(pattern = paste0("\\",dirhead,".*"),replacement = "", x = path)
wd_head <- paste0(path_ahead,dirhead)
setwd(wd_head)

# Now import functions related to directories.
# Note that the script cannot be called again once the directory is set to the 
# subdirectory of the project (unless setwd() is called again).
source("code/functions_set_wd.R")
source("code/functions_general.R")

# return the name of the directories for the different projects:
subDir_projects <- subDir_projects_fun()

wds_l <- set_working_directories_fun(subDir = subDir_projects$spawner_surveys,
                                     Export_locally = F)
wd_head <- wds_l$wd_head
wd_project <- wds_l$wd_project
wd_code <- wds_l$wd_code
wd_data <- wds_l$wd_data
wd_figures <- wds_l$wd_figures
wd_output <- wds_l$wd_output
wd_X_Drive1_PROJECTS <- wds_l$wd_X_Drive1_PROJECTS

wd_references_dropbox <- paste(wd_X_Drive1_PROJECTS,
                               wds_l$wd_project_dropbox,
                               "references",sep="/")

wd_data_dropbox <- paste(wd_X_Drive1_PROJECTS,
                         wds_l$wd_project_dropbox,
                         "data",sep="/")

wd_documents <- paste(wd_project,"documents",sep="/")

wd_pop_indic_data_input_dropbox <- paste(wd_X_Drive1_PROJECTS,
                                         wds_l$wd_population_indicator_data_input_dropbox,
                                         sep = "/")

wd_pop_indic_data_gis_dropbox <- gsub("input","gis",wd_pop_indic_data_input_dropbox)

# Loading packages & functions
library(tidyr)
library(dplyr) # for arrange()
library(sf)
library(sp)     # for spDists() TERRA is the replacement
library(scales)
library(readxl)

source("code/functions.R")


#'* !!! DECISIONS TO MAKE !!! *

# Set tp true to update the datasets
export_datasets <- F

#' Decision made in 1_nuseds_collation.rmd: 
#' Remove the IndexId & GFE_ID time series in NUSEDS with only NAs and/or 0s
#' If TRUE, time series only composed of NAs and/or 0s are removed from NUSEDS; 
#' If FALSE, time series only composed of NAs are removed from NUSEDS.
#' Setting to T or F will import the corresponding dataset.
remove_timeSeries_zeros_too <- T 

#' Decision made in 1_nuseds_collation.rmd: Used just after NUSEDS and CUSS are merged,
#'to decide if we want to replace all the remaining 0s by NAs.
#' Setting to T or F will import the corresponding dataset.
replace_zeros_byNAs <- T

```

# Import datasets

Import the cleaned NuSEDS dataset with the following decisions made concerning zeros:

```{r, include=FALSE}

#'* Import the log file *
logfile_path <- paste0(getwd(),"/spawner-surveys/output/log_file.csv")
logfile_path <- gsub("spawner-surveys/code/","",logfile_path)
log_file <- read.csv(logfile_path,header = T)

# selected the rows corresponding to the decision made concerning including or 
# not the 0s:
cond <- log_file$remove_timeSeries_zeros_too == remove_timeSeries_zeros_too & 
  log_file$replace_zeros_byNAs == replace_zeros_byNAs & 
  grepl("NuSEDS_escapement_data_collated",log_file$main_dataset_exported)
# select the most recent date:
date_export <- max(log_file[cond,]$date_export)


#'* Import the most recent NuSEDS_escapement_data_collated file *
# nuseds <- import_mostRecent_file_fun(wd = paste0(wd_output,"/archive"), 
#                                      pattern = "NuSEDS_escapement_data_collated")

nuseds <- read.csv(paste0(wd_output,"/archive/1_NuSEDS_escapement_data_collated_",date_export,".csv"),header = T)

head(nuseds)
nrow(nuseds) # 311566 310255 309338 306823 306999

nrow(unique(nuseds[,c("SPECIES_QUALIFIED","POP_ID","SYSTEM_SITE","WATERBODY")])) # 6972 6892 6854 6868
nrow(unique(nuseds[,c("SPECIES_QUALIFIED","POP_ID","SYSTEM_SITE","WATERBODY","GFE_ID")])) # 6972 6892 6854 6868

sum(nuseds$MAX_ESTIMATE == 0 & !is.na(nuseds$MAX_ESTIMATE)) # 0
sum(is.na(nuseds$MAX_ESTIMATE)) # 158986 157808 157264 155984
```

```{r,echo=FALSE}

if(remove_timeSeries_zeros_too){
  print("Time series only composed of NAs AND/OR zeros were removed from NUSEDS in the early cleaning process in 1_nuseds_collation.rmd.")
}else{
  print("Time series only composed of NAs were removed from NUSEDS in the early cleaning process in 1_nuseds_collation.rmd, zeros were left.")
}

if(replace_zeros_byNAs){
  print("Zeros were replaced by NAs in the late cleaning process in 1_nuseds_collation.rmd.")
}else{
  print("Zeros were NOT replaced by NAs in the late cleaning process in 1_nuseds_collation.rmd.")
}

```
The PSE file *conservationunits_decoder.csv* is imported; this is the file containing the current `cuid` associated to NuSEDS's `FULL_CU_IN` (named `cu_index` in the decoder file).

```{r,include=FALSE}

#'Import the name of the different datasets in the PSF database and their 
#' corresponding CSV files.
datasetsNames_database <- datasetsNames_database_fun()

fromDatabase <- update_file_csv <- F

#'* Import the conservationunits_decoder.csv *
#' from population-indicators/data_input or download it from the PSF database.
#' # To obtain the generation length and calculate the the "current spawner abundance".
conservationunits_decoder <- datasets_database_fun(nameDataSet = datasetsNames_database$name_CSV[1],
                                                   fromDatabase = fromDatabase,
                                                   update_file_csv = update_file_csv,
                                                   wd = wd_pop_indic_data_input_dropbox)

head(conservationunits_decoder)

# check the CUs with is.na(cu_index):
sum(is.na(conservationunits_decoder$cu_index))    # 45
sum(is.na(conservationunits_decoder$cu_name_pse))
cond <- is.na(conservationunits_decoder$cu_index)
conservationunits_decoder[cond,]

# temporary fix to do (until the source file is corrected)
d_fixes <- data.frame(cuid = c(528,756,758,759,760,761,763,1022),
                      cu_index_old = c("SX_528","","","","","","","SER-023"),
                      cu_index_new = c("SEL-16-01","SEL-03-07","SEL-05-01","SEL-06-19",
                                       "SEL-09-04","SEL-09-05","SEL-10-02","SER-23"))

for(r in 1:nrow(d_fixes)){
  cuid <- d_fixes$cuid[r]
  cu_index_new <- d_fixes$cu_index_new[r]
  cond <- conservationunits_decoder$cuid == cuid
  conservationunits_decoder$cu_index[cond] <- cu_index_new
}

d_fixes
```

Import the *streamlocationids.csv* to access the location names, coordinates and identification number for maintaining consistency.

```{r, echo=FALSE}
#'* Files from PSF database *

# NOT DONE FOR NOW:
# **TEMPORARY**: change the name of the field `streamid` to `populationid` because the latter is the unique association between a CU (`cuid`) and a location/stream (`GFE_ID`).

#'Import the name of the different datasets in the PSF database and their 
#' corresponding CSV files.
datasetsNames_database <- datasetsNames_database_fun()

fromDatabase <- update_file_csv <- F

#' Import streamlocationids to obtain the streamID 
streamlocationids <- datasets_database_fun(nameDataSet = "streamlocationids.csv", # datasetsNames_database$name_CSV[9],
                                           fromDatabase = fromDatabase,
                                           update_file_csv = update_file_csv,
                                           wd = wd_pop_indic_data_input_dropbox)

streamlocationids$sys_nm <- gsub("\\\\","'",streamlocationids$sys_nm)
streamlocationids$sys_nm <- gsub("  "," ",streamlocationids$sys_nm)

rownames(streamlocationids) <- NULL
# colnames(streamlocationids)[colnames(streamlocationids) == "streamid"] <- "populationid"

# add the field region from the decoder
streamlocationids$region <- NA
for(rg_i in unique(streamlocationids$regionid)){
  cond <- conservationunits_decoder$drv_regionid == rg_i
  region <- unique(conservationunits_decoder$region[cond])
  
  cond <- streamlocationids$regionid == rg_i
  streamlocationids$region[cond] <- region
}

# remove unnecessary and confusing fields
streamlocationids <- streamlocationids[,c("region","sys_nm","GFE_ID","latitude","longitude","cu_name_pse","cuid","streamid")]
rownames(streamlocationids) <- NULL

head(streamlocationids)
```

Import the regions' shape files. These represent the geographic boundaries as displayed in the PSE.

```{r, include=FALSE}
#'* Import the shape files for the Region boundaries  *
# wd_maps_rg <- gsub("1_PROJECTS","5_DATA",wd_X_Drive1_PROJECTS) # files not up to date
# wd_maps_rg <- gsub("1_PROJECTS","5_DATA",wd_X_Drive1_PROJECTS)
# wd_maps_rg <- paste0(wd_pop_indic_data_gis_dropbox,"/se_boundary_regions")
regions_shp <- st_read(paste0(wd_pop_indic_data_gis_dropbox,"/se_boundary_regions/se_boundary_regions.shp")) %>%
  st_transform(crs = 4269)
unique(regions_shp$regionname)
sf_use_s2(FALSE) # so that st_intersects() and st_simplify() can be used
regions_shp_full <- regions_shp
regions_shp <- st_simplify(x = regions_shp, dTolerance = .002) # .001 # to reduce computation time

```

# Modifications

## Add the field StatArea

This new `StatArea` field is equivalent to the original field `AREA` with the following changes made:

```{r, echo=FALSE}

# Note: this was done in script 1 previously but messed up with the correction process because
# StatArea is a - GFE_ID field but the change below are done for certain populations which creates a 
# bug.

#'* Add the field StatArea to NUSEDS * 
#' Old code, not sure how important this is.
nuseds$StatArea <- Convert2StatArea(nuseds$AREA)

# remove the 0 that were added in Convert2StatArea so we can compare to AREA
# StatArea_compare <-  nuseds$StatArea
# StatArea_compare <- sapply(StatArea_compare,function(c){
#   if(substring(c, 1, 1) == "0"){
#     out <- substring(c, 2)
#   }else{
#     out <- c
#   }
#   return(out)
# })
cond <- nuseds$AREA != nuseds$StatArea
show <- unique(nuseds[cond,c("AREA","StatArea")])
show <- show[order(show$AREA),]
rownames(show) <- NULL
show
```

We also make the following changes for certain `IndexId/POP_ID`:

```{r}
# Make corrections for populations with discrepancies in area assignments #
# These errors become apparent when merging data frames later on #
nuseds[nuseds$IndexId == "CO_46240",]$StatArea <- "29"    # vs. "29J" "29K"
nuseds[nuseds$IndexId == "PKO_51094",]$StatArea <- "12"  # BSC: there is one ""
nuseds[nuseds$IndexId == "SX_45495",]$StatArea <- "120"  # BSC: already "120"

cond <- nuseds$IndexId %in% c("CO_46240","PKO_51094","SX_45495")
cond2 <- nuseds$AREA != nuseds$StatArea

show <- nuseds[cond & cond2,c("IndexId","AREA","StatArea")]|> unique()
rownames(show) <- NULL
show
```

## Add the field FULL_CU_IN_PSE

The `FULL_CU_IN` of several population is updated to reflect (1) partition of the Sockeye CU `FULL_CU_IN` = "SEL-21-02" into sub-groups (EW: 'Early Wild', for Babine/Onerka; LW: 'Late Wild' for Nilkitkwa, and F: Fulton and Pinkut) as in [DFO 2023](https://waves-vagues.dfo-mpo.gc.ca/library-bibliotheque/41102356.pdf); (2) recent corrections for the Bella Coola River-Late CU (CM-17) and Bella Coola-Dean Rivers (CM-16) (personal communication from Carrie Holt, DFO, May 2023). The field `FULL_CU_IN_PSE` is created to reflect these changes.

```{r, echo=FALSE}
#'* Edit FULL_CU_IN for several POP_IDs * 
# Corrections in CU assignment for central coast chum from Carrie Holt
# https://salmonwatersheds.slack.com/archives/C017N5NSCJY/p1683774240661029?thread_ts=1683735939.696999&cid=C017N5NSCJY
# https://salmonwatersheds.slack.com/archives/CJ5RVHVCG/p1705426563165399?thread_ts=1705344122.088409&cid=CJ5RVHVCG

nuseds$FULL_CU_IN_PSE <- nuseds$FULL_CU_IN

#' Import the corrections:
full_cu_l <- update_for_FULL_CU_IN_l()

show <- NULL
for(i in 1:length(full_cu_l)){
  # i <- 5
  FULL_CU_IN_here <- names(full_cu_l)[i]
  # print(FULL_CU_IN_here)
  
  POP_IDs_here <- full_cu_l[[i]]
  
  #
  cond_nuseds <- nuseds$POP_ID %in% POP_IDs_here
  
  if(any(cond_nuseds)){
    show_here <- unique(nuseds[cond_nuseds,c("POP_ID","FULL_CU_IN")])
  
    CU_NAME <- unique(nuseds[cond_nuseds,c("CU_NAME")])
    
    cond_decoder <- conservationunits_decoder$cu_index == FULL_CU_IN_here &
      !is.na(conservationunits_decoder$cu_index)
  
    if(!any(cond_decoder)){
      print("not match for FULL_CU_IN_here in decoder - BREAL")
      break
    
    }else{
      show_here$region <- unique(conservationunits_decoder$region[cond_decoder])
      show_here$species_name <- unique(conservationunits_decoder$species_name[cond_decoder])
      show_here$CU_NAME <- CU_NAME
      show_here$cu_name_pse <- unique(conservationunits_decoder$cu_name_pse[cond_decoder])
      show_here$FULL_CU_IN_PSE <- FULL_CU_IN_here
      show_here <- show_here[,c("region","species_name","CU_NAME","cu_name_pse","FULL_CU_IN","FULL_CU_IN_PSE")] |> unique()
      rownames(show_here) <- NULL
    
      if(is.null(show)){
        show <- show_here
      }else{
        show <- rbind(show,show_here)
      }
      
      nuseds$FULL_CU_IN_PSE[cond_nuseds] <- FULL_CU_IN_here
    }
  }
}

show
```

## CU_NAME updates

Additionally, the `CU_NAME` of the river Sockeye at `SYSTEM` = "BELLA COOLA RIVER" (`GFE_ID` = 968) is changed from:

```{r, echo=FALSE}
#'* FIX: South Atnarko Lakes *
#' GFE_ID 968 for sockeye should be attributed to South Atnarko Lakes CU 
#' (cf. Population meeting from 05/03/2024)
cond <- nuseds$GFE_ID == 968 & nuseds$SPECIES_QUALIFIED %in% c("SEL","SER")
# unique(nuseds$CU_NAME[cond]) # "NORTHERN COASTAL FJORDS"
# unique(nuseds$SPECIES_QUALIFIED[cond]) # SER
# unique(nuseds$SYSTEM_SITE[cond]) # "BELLA COOLA RIVER"

show <- unique(nuseds[cond,c("SPECIES_QUALIFIED","CU_NAME","SYSTEM_SITE","GFE_ID")])
rownames(show) <- NULL
show
```

to:

```{r, echo=FALSE}
nuseds$CU_NAME[cond] <- toupper("South Atnarko Lakes")
show <- unique(nuseds[cond,c("SPECIES_QUALIFIED","CU_NAME","SYSTEM_SITE","GFE_ID")])
rownames(show) <- NULL
show
```

## Add the field stream_survey_quality

We create the field `stream_survey_quality` from `ESTIMATE_CLASSIFICATION` such as:

```{r, echo=FALSE}

#'* Create stream_survey_quality from ESTIMATE_CLASSIFICATION *
#' cf. Table 4.5 in section 4.1.3 of the Tech Report
estim_class_nuseds <- unique(nuseds$ESTIMATE_CLASSIFICATION)

nuseds$stream_survey_quality <- NA
for(ecn in estim_class_nuseds){
  # ecn <- estim_class_nuseds[1]
  cond_nuseds <- nuseds$ESTIMATE_CLASSIFICATION == ecn
  
  if(ecn == "TRUE ABUNDANCE (TYPE-1)"){
    out <- "High"
  }else if(ecn == "TRUE ABUNDANCE (TYPE-2)"){
    out <- "Medium-High"
  }else if(ecn == "RELATIVE ABUNDANCE (TYPE-3)"){
    out <- "Medium"
  }else if(ecn == "RELATIVE ABUNDANCE (TYPE-4)"){
    out <- "Medium-Low"
  }else if(ecn %in% c("RELATIVE ABUNDANCE (TYPE-5)",
                      "RELATIVE: CONSTANT MULTI-YEAR METHODS")){
    out <- "Low"
  }else if(ecn %in% c("PRESENCE/ABSENCE (TYPE-6)",
                      "PRESENCE-ABSENCE (TYPE-6)",
                      "RELATIVE: VARYING MULTI-YEAR METHODS")){
    out <- "Low"
  }else if(ecn == "UNKNOWN"){
    out <- "Unknown"
  }else if(ecn %in% c("","NO SURVEY THIS YEAR","NO SURVEY")){
    out <- NA
  }else{
    print(ecn)
  }
  #print(out)
  nuseds$stream_survey_quality[cond_nuseds] <- out
}

# survey_score:
# cf. Table 4.5 in section 4.1.3 of the Tech Report
# nuseds <- import_mostRecent_file_fun(wd = paste0(wd_output,"/archive"),
#                                      pattern = "nuseds_cuid_streamid_")
# estim_class_nuseds <- unique(nuseds$ESTIMATE_CLASSIFICATION)
# estim_class_nuseds
# 
# nuseds$survey_score <- NA
# for(ecn in estim_class_nuseds){
#   # ecn <- estim_class_nuseds[1]
#   cond_nuseds <- nuseds$ESTIMATE_CLASSIFICATION == ecn
#   
#   if(ecn == "TRUE ABUNDANCE (TYPE-1)"){
#     out <- "1"
#   }else if(ecn == "TRUE ABUNDANCE (TYPE-2)"){
#     out <- "2"
#   }else if(ecn == "RELATIVE ABUNDANCE (TYPE-3)"){
#     out <- "3"
#   }else if(ecn == "RELATIVE ABUNDANCE (TYPE-4)"){
#     out <- "4"
#   }else if(ecn %in% c("RELATIVE ABUNDANCE (TYPE-5)",
#                       "RELATIVE: CONSTANT MULTI-YEAR METHODS")){
#     out <- "5"
#   }else if(ecn %in% c("PRESENCE/ABSENCE (TYPE-6)",
#                       "PRESENCE-ABSENCE (TYPE-6)",
#                       "RELATIVE: VARYING MULTI-YEAR METHODS")){
#     out <- "6"
#   }else if(ecn == "UNKNOWN"){
#     out <- "Unknown"
#   }else if(ecn %in% c("","NO SURVEY THIS YEAR","NO SURVEY")){
#     out <- NA
#   }else{
#     print(ecn)
#   }
#   #print(out)
#   nuseds$survey_score[cond_nuseds] <- out
# }
# 
# nuseds$survey_score |> unique()

show <- unique(nuseds[,c("ESTIMATE_CLASSIFICATION","stream_survey_quality")])
rownames(show) <- NULL

ESTIMATE_CLASSIFICATION <- c("TRUE ABUNDANCE (TYPE-1)",
                             "TRUE ABUNDANCE (TYPE-2)",
                             "RELATIVE ABUNDANCE (TYPE-3)",
                             "RELATIVE ABUNDANCE (TYPE-4)",
                             "RELATIVE ABUNDANCE (TYPE-5)",
                             "PRESENCE-ABSENCE (TYPE-6)",
                             "RELATIVE: CONSTANT MULTI-YEAR METHODS",
                             "RELATIVE: VARYING MULTI-YEAR METHODS",
                             "UNKNOWN",
                             "NO SURVEY THIS YEAR")

order <- order(factor(x = show$ESTIMATE_CLASSIFICATION, levels = ESTIMATE_CLASSIFICATION))
show <- show[order,]
rownames(show) <- NULL
show
```

## Fixes on ESTIMATE_METHOD 

We make the following corrections to the field `ESTIMATE_METHOD`:

```{r, echo=FALSE}

#'* Fixes in the methods *
# Katy's request:
# https://salmonwatersheds.slack.com/archives/C03LB7KM6JK/p1712611492405689?thread_ts=1712252256.802999&cid=C03LB7KM6JK

# unique(nuseds$ESTIMATE_METHOD)

estMeth <- data.frame(ESTIMATE_METHOD = c("Cummulative","Unknown","Fixed Site Census",
                                          "Aerial","Fence", "Insufficient Information"),
                      correction = c("Cumulative","Unknown Estimate Method","Fence Count",
                                     "Aerial Survey","Fence Count","Unknown Estimate Method"))

for(r in 1:nrow(estMeth)){
  # r <- 2
  if(estMeth$ESTIMATE_METHOD[r] == "Cummulative"){
    nuseds$ESTIMATE_METHOD <- gsub("Cummulative","Cumulative",nuseds$ESTIMATE_METHOD)
  }else{
    cond <- nuseds$ESTIMATE_METHOD == estMeth$ESTIMATE_METHOD[r]
    nuseds$ESTIMATE_METHOD[cond] <- estMeth$correction[r]
  }
}

estMeth
```

# Find the cuid from FULL_CU_IN_PSE

We associate the previously defined field `FULL_CU_IN_PSE` to the PSE's `cuid`, `cu_name_pse`, `cu_name_dfo`, `region`, using the *conservationunits_decoder.csv*. For instance:

```{r, include=FALSE}

#'* Provide a cuid to each row in nuseds using conservationunits_decoder *
#' using: FULL_CU_IN_PSE

nuseds$cuid <- NA
nuseds$cu_name_pse <- NA
nuseds$cu_name_dfo <- NA
nuseds$region <- NA
nuseds$regionid <- NA

cuid_cu_index <- unique(unique(nuseds[,c("CU_NAME","FULL_CU_IN_PSE")]))
nrow(cuid_cu_index)
cuid_cu_index$cuid <- NA
for(r in 1:nrow(cuid_cu_index)){
  fci <- cuid_cu_index$FULL_CU_IN_PSE[r]
  cond_nuseds <- nuseds$FULL_CU_IN_PSE == fci
  
  if(fci ==  "No FULL_CU_in but cuid = 242"){
    # there is one instance where a time series (one data point) could not be associated to a FULL_CU_IN in CUSS
    # because the Cu is simply not present, but it is in the PSE
    cond_decoder <- conservationunits_decoder$cuid == 242
  }else{
    cond_decoder <- conservationunits_decoder$cu_index == fci & !is.na(conservationunits_decoder$cu_index)
  }
  
  if(any(cond_decoder)){
    cuid_cu_index$cuid[r] <- unique(conservationunits_decoder$cuid[cond_decoder])
    
    nuseds$cuid[cond_nuseds] <- unique(conservationunits_decoder$cuid[cond_decoder])
    nuseds$cu_name_pse[cond_nuseds] <- unique(conservationunits_decoder$cu_name_pse[cond_decoder])
    nuseds$cu_name_dfo[cond_nuseds] <- unique(conservationunits_decoder$cu_name_dfo[cond_decoder])
    nuseds$region[cond_nuseds] <- unique(conservationunits_decoder$region[cond_decoder])
  }
}
```

```{r, echo=FALSE}
show <- unique(nuseds[,c("region","SPECIES","CU_NAME","cu_name_dfo","cu_name_pse","FULL_CU_IN_PSE","cuid")])

rownames(show) <- NULL
head(show)
```

```{r, include=FALSE}
#'* Check the FULL_CU_IN not in the decoder: *
cond_NA <- is.na(cuid_cu_index$cuid)
head(cuid_cu_index)
sum(cond_NA) # 23

cu_index_NA <- cuid_cu_index[cond_NA,]
colnames(cu_index_NA)[colnames(cu_index_NA) == "FULL_CU_IN_PSE"] <- "FULL_CU_IN"

rownames(cu_index_NA) <- NULL
```

There are `r nrow(cu_index_NA)` `FULL_CU_IN` that are not in *conservationunits_decoder.csv*. We use the regions' shape file to find their respective region:

```{r,echo=FALSE}

# Find the coordinates
cu_index_NA$CU_LAT <- sapply(cu_index_NA$FULL_CU_IN,function(cui){
  cond <- nuseds$FULL_CU_IN == cui
  return(unique(nuseds$CU_LAT[cond]))
})

cu_index_NA$CU_LONGT <- sapply(cu_index_NA$FULL_CU_IN,function(cui){
  cond <- nuseds$FULL_CU_IN == cui
  return(unique(nuseds$CU_LONGT[cond]))
})

#' Find the corresponding region:
cu_index_NA$region <- NA
for(r in 1:nrow(cu_index_NA)){
  # r <- 1
  point <- st_as_sf(cu_index_NA[r,], 
                    coords = c("CU_LONGT","CU_LAT"), crs = 4269)
  
  layer_rg <- st_intersects(point, regions_shp)
  if(length(layer_rg[[1]]) == 0){ # no match, try to buffer
    layer_rg <- st_intersects(point, st_buffer(x = regions_shp, dist = .001))
  }
  
  if(length(layer_rg[[1]]) == 0){ # no match, try to buffer
    layer_rg <- st_intersects(point, st_buffer(x = regions_shp, dist = .002))
  }
  
  if(length(layer_rg[[1]]) == 0){ # no match, try to buffer
    print("Still no region found - BREAK")
    break
  }
  
  if(length(layer_rg[[1]]) > 1){ # 
    print("More than one region found - BREAK")
    break
  }
  
  layer_rg <- layer_rg[[1]]
  cu_index_NA$region[r] <- regions_shp$regionname[layer_rg]
}

cu_index_NA
```

```{r, include=FALSE}
# used to be cuid 751
cond_751 <- grepl(simplify_string_fun("Adams and Momich Lakes_Early Summer"),
              simplify_string_fun(nuseds$CU_NAME))
nuseds[cond_751,]$WATERBODY |> unique()
nuseds[cond_751,]$CU_NAME |> unique()
nuseds[cond_751,]$FULL_CU_IN |> unique()

nuseds[cond_751 , c("cuid","WATERBODY","Y_LAT","X_LONGT")] |> unique()
#  cuid            WATERBODY latitude_final longitude_final
#   751         BURTON CREEK       51.48255       -119.4648 --> 760  Adams-Early Summer
#   751 MOMICH RIVER - UPPER       51.31958       -119.3236 --> 761 Momich-Early Summer
#   751  ADAMS RIVER - UPPER       51.41090       -119.4561 --> 760  Adams-Early Summer
#   751         MOMICH RIVER       51.33461       -119.4232 --> 761 Momich-Early Summer
#   751        CAYENNE CREEK       51.32071       -119.3197 --> 761 Momich-Early Summer
```

The CU above with `FULL_CU_IN` = `r unique(nuseds[cond_751,]$FULL_CU_IN)` is split into the two following CUs to reflect COSEWIC's grouping (note the creation of the field `cu_name_pse`):

```{r, echo=FALSE}
# UPDATE: 2024-11-21: the CU Fraser Sockeye Adams & Momich Lakes-Early Summer  
# 751 was split into the CUs with cuid 760 and 761. Consequently cuid 751 is not
# in the decoder anymore. For the sake of updating the data for the data demise
# paper, I add it here:
# This section should be removed in the next NuSEDS update.
# CU Fraser Sockeye Adams & Momich Lakes-Early Summer  751 that was split into CUs 760 and 761
# https://salmonwatersheds.slack.com/archives/C03LB7KM6JK/p1725753313593719?thread_ts=1725564850.867719&cid=C03LB7KM6JK
# cf. population meeting September 11 2024

cond_760 <- cond_751 & nuseds$WATERBODY %in% c("BURTON CREEK","ADAMS RIVER - UPPER")
cond_761 <- cond_751 & nuseds$WATERBODY %in% c("MOMICH RIVER - UPPER","MOMICH RIVER",
                                               "CAYENNE CREEK")

nuseds$cuid[cond_760] <- 760
nuseds$cuid[cond_761] <- 761

for(cuid in 760:761){
  cond_nuseds <- nuseds$cuid == cuid & !is.na(nuseds$cuid)
  cond_decoder <- conservationunits_decoder$cuid == cuid
  nuseds$cu_name_pse[cond_nuseds] <- conservationunits_decoder$cu_name_pse[cond_decoder]
  nuseds$cu_name_dfo[cond_nuseds] <- conservationunits_decoder$cu_name_dfo[cond_decoder]
  nuseds$FULL_CU_IN_PSE[cond_nuseds] <- conservationunits_decoder$cu_index[cond_decoder]
  nuseds$region[cond_nuseds] <- conservationunits_decoder$region[cond_decoder]
}

show <- nuseds[cond_751,c("FULL_CU_IN","FULL_CU_IN_PSE","CU_NAME","cu_name_pse","cuid","WATERBODY")] |> unique()
rownames(show) <- NULL
show[order(show$cuid),]

# cond_751 <- grepl(simplify_string_fun("Adams and Momich Lakes_Early Summer"),
#               simplify_string_fun(nuseds$CU_NAME))
# nuseds[cond_751,]$cuid <- 751
# nuseds[cond_751,]$cu_name_pse <- "Adams & Momich Lakes-Early Summer"
# nuseds[cond_751,]$cu_name_dfo <- "Adams and Momich Lakes_Early Summer <<Extinct>>"
# nuseds[cond_751,]$regionid <- 4
# nuseds[cond_751,]$region <- "Fraser"

```

We show below the corresponding time series for the rest of the unmatched CUs:

```{r, echo=FALSE, fig.height=7.5, fig.width=10}
# update cu_index_NA
cond <- grepl(simplify_string_fun("Adams and Momich Lakes_Early Summer"),
              simplify_string_fun(cu_index_NA$CU_NAME))
cu_index_NA <- cu_index_NA[!cond,]

# produce figure time series
for(r in 1:nrow(cu_index_NA)){
  # r <- 1
  FULL_CU_IN <- cu_index_NA$FULL_CU_IN[r]
  cond <- nuseds$FULL_CU_IN == FULL_CU_IN
  IndexId_GFE_ID <- unique(nuseds[cond,c("IndexId","GFE_ID")])
  
  main <- paste(cu_index_NA$CU_NAME[r],
                cu_index_NA$FULL_CU_IN[r],
                sep = " - ")
  
  plot_IndexId_GFE_ID_fun(IndexIds = IndexId_GFE_ID$IndexId,
                          GFE_IDs = IndexId_GFE_ID$GFE_ID,
                          all_areas_nuseds = nuseds, 
                          main = c(cu_index_NA$region[r],main))
  legend("topright",paste0("r = ",r), bty = "n")
}
```

These time series are kept in the dataset but will be removed in the next step/script.

# Work on locations

## Corrections

There are two `LOCAL_NAME_1` with typos, which we replace by the value in `SYSTEM_SITE`:

```{r, echo=FALSE}
# Manual fix 1st:
#'-  "BARRI\xc8RE RIVER" --> "BARRIERE RIVER"
#'- "FRAN\xc7OIS LAKE" --> FRANCOIS LAKE"

cond1 <- nuseds$SYSTEM_SITE == "BARRIERE RIVER"
cond2 <- nuseds$SYSTEM_SITE == "FRANCOIS LAKE"

show <- unique(nuseds[cond1 | cond2,c("SYSTEM_SITE","WATERBODY","LOCAL_NAME_1","LOCAL_NAME_2")])
rownames(show) <- NULL

nuseds$LOCAL_NAME_1[cond1] <- "BARRIERE RIVER"
nuseds$LOCAL_NAME_1[cond2] <- "FRANCOIS LAKE"

show
```

We also create the field `sys_nm`, which is equivalent of `SYSTEM_SITE` but the following corrections or modification for the following locations:

```{r, include = FALSE}

# Most changes are in streamlocationids

# some changes are also in this list:
SYSTEM_SITE_fixes <- SYSTEM_SITE_fixes_fun()

SYSTEM_SITE_fixes <- data.frame(SYSTEM_SITE = SYSTEM_SITE_fixes$SYSTEM_SITE,
                                sys_nm = SYSTEM_SITE_fixes$sys_nm)

SYSTEM_SITE_fixes$sys_nm <- gsub('\\\\',"",SYSTEM_SITE_fixes$sys_nm)

# Only retain the SYSTEM_SITE in nuseds (all of them should be but just in case)
cond <- SYSTEM_SITE_fixes$SYSTEM_SITE %in% nuseds$SYSTEM_SITE
SYSTEM_SITE_fixes <- SYSTEM_SITE_fixes[cond,]

# Select all the locations in nuseds
nuseds_streams <- unique(nuseds[,c("SYSTEM_SITE","GFE_ID")])
# nrow(nuseds_streams) # 2325

# Add the corresponding streamlocationids$sys_nm
nuseds_streams$sys_nm <- sapply(nuseds_streams$GFE_ID, function(gfeid){
  cond <- streamlocationids$GFE_ID == gfeid & !is.na(streamlocationids$GFE_ID)
  if(!any(cond)){
    out <- NA
  }else{
    out <- unique(streamlocationids$sys_nm[cond])
  }
  
  if(length(out) > 1){
    out <-  paste0(out, collapse = "; ")
    out <- paste0("MORE THAN ONE LOCATION: ",out)
  }
  
  return(out)
})

# Retain the cases where there is a difference in the name:
cond <- !is.na(nuseds_streams$sys_nm) &
  simplify_string_fun(nuseds_streams$sys_nm) != simplify_string_fun(nuseds_streams$SYSTEM_SITE)
change1 <- nuseds_streams[cond,]

# Sites in SYSTEM_SITE_fixes not in streamlocationids
cond <- SYSTEM_SITE_fixes$sys_nm %in% streamlocationids$sys_nm
sum(!cond)
SYSTEM_SITE_fixes[!cond,]

# retain those in nuseds
cond2 <- SYSTEM_SITE_fixes[!cond,]$SYSTEM_SITE %in% nuseds$SYSTEM_SITE
change2 <-  SYSTEM_SITE_fixes[!cond,][cond2,]

# find GFE_ID
change2$GFE_ID <- sapply(change2$SYSTEM_SITE,function(ss){
  cond <- nuseds$SYSTEM_SITE == ss
  return(unique(nuseds$GFE_ID[cond]))
})
change2 <- change2[,c("GFE_ID","SYSTEM_SITE","sys_nm")]

# check if there is any change2$SYSTEM_SITE in change1$SYSTEM_SITE
cond <- change2$SYSTEM_SITE %in% change1$SYSTEM_SITE
change2[cond,] # if no all good

# change2$sys_nm_2 <-  sapply(change2$SYSTEM_SITE,function(ss){
#   cond <- change1$SYSTEM_SITE == ss
#   out <- NA
#   if(any(cond)){
#     out <- change1$sys_nm[cond]
#   }
#   return(out)
# })

# Combine the two
cols <- c("GFE_ID","SYSTEM_SITE","sys_nm")
change_all <- rbind(change1[,cols],change2[,cols])
change_all <- change_all[order(change_all$SYSTEM_SITE),]
rownames(change_all) <- NULL
```

```{r, echo=FALSE}
show <- change_all[, c("SYSTEM_SITE","sys_nm","GFE_ID")]
rownames(show) <- NULL
show
```


```{r, include=FALSE}
# add the field the nuseds
nuseds$sys_nm <- nuseds$SYSTEM_SITE
for(r in 1:nrow(change_all)){
  cond <- nuseds$GFE_ID == change_all$GFE_ID[r]
  
  if(any(cond)){ # should not need this security
    nuseds$sys_nm[cond] <- change_all$sys_nm[r]
  }
}
```

##  Attribute streamid to nuseds

The field `streamid` is a unique combination between a stream (`GFE_ID`) and CU (`cuid`). We use the PSE file *streamlocationids.csv* to match the existing `streamid`, for instance: 

```{r, echo=FALSE}
show <- head(streamlocationids)
rownames(show) <- NULL
show
```

We match the locations using `GFE_ID` if available in **streamlocationids**, and `SYSTEM_SITE`, `Y_LAT` and `X_LONGT` otherwise. In case of the latter, the matched locations are shown below:

```{r, echo = FALSE}
nuseds$streamid <- NA

for(r in 1:nrow(streamlocationids)){
  # r <- 1
  
  cond_cuid <- nuseds$cuid == streamlocationids$cuid[r] & !is.na(nuseds$cuid)
  cond_location <- rep(F,length(cond_cuid))
  
  if(!is.na(streamlocationids$GFE_ID[r])){
    cond_location <- nuseds$GFE_ID == streamlocationids$GFE_ID[r] &
      !is.na(nuseds$cuid) & 
      nuseds$cuid == streamlocationids$cuid[r]
    
  }else{ # try matching with name of the system and coordinates
    cond_location <- simplify_string_fun(nuseds$SYSTEM_SITE) == simplify_string_fun(streamlocationids$sys_nm[r]) &
      round(nuseds$Y_LAT,4) == round(streamlocationids$latitude[r],4) &
      round(nuseds$X_LONGT,4) == round(streamlocationids$longitude[r],4)
    
    if(any(cond_location)){

      show1 <- nuseds[cond_location,c("SYSTEM_SITE","GFE_ID","Y_LAT","X_LONGT")] |> unique()
      show2 <- streamlocationids[r,c("sys_nm","GFE_ID","latitude","longitude")] |> unique()
        
      colnames(show1) <- c("location","GFE_ID","latitude","longitude")
      colnames(show2) <- c("location","GFE_ID","latitude","longitude")
        
      show <- rbind(show1,show2)
      show$dataset <- c("NuSEDS","streamlocationids")
      show <- show[,c("dataset","location","GFE_ID","latitude","longitude")]
      rownames(show) <- NULL
      print(show)
      print("")
    }
  }
  
  if(any(cond_cuid & cond_location)){
    nuseds$streamid[cond_cuid & cond_location] <- streamlocationids$streamid[r]
  }
}

check <- unique(nuseds[,c("region","GFE_ID","sys_nm","cuid","cu_name_pse","streamid")])
# nrow(check) # 6959 6880
```

There remains `r sum(is.na(check$streamid))` unique stream (`GFE_ID`) and CU (`cuid`) combinations without an existing `streamid` (not accounting for the time series in **nuseds** without a `cuid`):

```{r, echo=FALSE}
cond_NA <- is.na(check$streamid) & !is.na(check$cuid)
show <- check[cond_NA,]
show <- show[order(show$region),]
rownames(show) <- NULL
show
```

We give them a new `streamid` value by simply incrementing from the maximum current `streamid` value.

```{r, include=FALSE}
val <- max(streamlocationids$streamid)
for(r in 1:nrow(show)){
  val <- val + 1
  show$streamid[r] <- val
  
  cond <- nuseds$GFE_ID == show$GFE_ID[r] &
    nuseds$cuid == show$cuid[r] & !is.na(nuseds$cuid)
  
  nuseds$streamid[cond] <- show$streamid[r] 
}
```


# Export datasets

The following files are exported:

- *2_nuseds_cuid_streamid__DATE.csv*: the cleaned NuSEDS dataset with the PSE's field: `cuid`, `streamid`, `cu_name_pse`, etc.

- *log_file.csv*: the log file reporting the name of the main file exported, the date of the export, the name of the present script, the name of the original **NUSEDS** or **CUSS** files and the choices related to removing zeros or not.

```{r, include=FALSE}
if(export_datasets){
  
  date <- as.character(Sys.Date())

  name_file <- paste0("2_nuseds_cuid_streamid_",date)
  write.csv(nuseds,paste0(wd_output,"/archive/",name_file,".csv"),
            row.names = F)

  # Edit the log file locally (needs to be pushed to github)
  logfile_path <- paste0(getwd(),"/spawner-surveys/output/log_file.csv")
  logfile_path <- gsub("spawner-surveys/code/","",logfile_path)
  log_file <- read.csv(logfile_path,header = T)
  
  cond_date_export <- log_file$date_export == date_export
  
  log_file_new <- log_file[1,]
  log_file_new$main_dataset_exported <- name_file
  log_file_new$R_script <- "2_nuseds_cuid_pse.rmd"
  log_file_new$date_export <- date
  log_file_new$all_areas_nuseds_file_name <- log_file$all_areas_nuseds_file_name[cond_date_export]
  log_file_new$conservation_unit_system_sites_file_name <- log_file$conservation_unit_system_sites_file_name[cond_date_export]
  log_file_new$remove_timeSeries_zeros_too <- log_file$remove_timeSeries_zeros_too[cond_date_export]
  log_file_new$replace_zeros_byNAs <- log_file$replace_zeros_byNAs[cond_date_export]
  
  log_file <- rbind(log_file,log_file_new)
  
  wd_output_local <- getwd()
  wd_output_local <- gsub("code","output",wd_output_local)
  
  write.csv(log_file,paste0(wd_output_local,"/log_file.csv"),row.names = F)
  
}
```

# Check: compared with previous version

```{r, echo=FALSE}
# selected the rows corresponding to the decision made concerning including or 
# not the 0s:
cond <- log_file$remove_timeSeries_zeros_too == remove_timeSeries_zeros_too & 
  log_file$replace_zeros_byNAs == replace_zeros_byNAs & 
  grepl("nuseds_cuid_streamid",log_file$main_dataset_exported)

# select the second most recent date:
dates <- log_file[cond,]$date_export
if(length(dates) == 1){
  date_2 <- sort(dates, decreasing = T)[1]
}else{
  date_2 <- sort(dates, decreasing = T)[2]
}

date_2 <- "2024-04-19"

#'* Import the previous NuSEDS_escapement_data_collated file *
nuseds_old <- read.csv(paste0(wd_output,"/archive/2_nuseds_cuid_streamid_",date_2,".csv"),header = T)
#colnames(nuseds_old)[colnames(nuseds_old) == "streamid"] <- "streamid"

#'* Import the last 1_series_added_DATE *
series_added_new <- read.csv(paste0(wd_output,"/archive/1_series_added_",date_export,".csv"), header = T)

#'* Import the previous 1_series_added_DATE *
series_added_old <- read.csv(paste0(wd_output,"/archive/series_added_",gsub("-","",date_2),".csv"), header = T)

#'* Import the last 1_series_removed_DATE *
series_removed_new <- read.csv(paste0(wd_output,"/archive/1_series_removed_",date_export,".csv"), header = T)

#'* Import the previous 1_series_added_DATE *
series_removed_old <- read.csv(paste0(wd_output,"/archive/series_removed_",gsub("-","",date_2),".csv"), header = T)
```

Import the previous clean NuSEDS dataset, exported in `r date_2`.

```{r, echo=FALSE}
cond_na <- is.na(nuseds$cuid)
timeseries_new <- unique(nuseds[!cond_na,c("IndexId","cuid","GFE_ID")])
# nrow(timeseries_new) # 6810
timeseries_new$IndexId_GFE_ID <- paste(timeseries_new$IndexId,timeseries_new$GFE_ID, sep = "_")

cond_na <- is.na(nuseds_old$cuid)
timeseries_old <- unique(nuseds_old[!cond_na,c("IndexId","cuid","GFE_ID")])
# nrow(timeseries_old) # 6766
timeseries_old$IndexId_GFE_ID <- paste(timeseries_old$IndexId,timeseries_old$GFE_ID, sep = "_")

if(nrow(timeseries_new) > nrow(timeseries_old)){
  print(paste("There are",nrow(timeseries_new) - nrow(timeseries_old),"more time series in the updated cleaned NuSEDS dataset"))
}else if(nrow(timeseries_new) < nrow(timeseries_old)){
  print(paste("There are",nrow(timeseries_old) - nrow(timeseries_new),"less tinme series in the updated cleaned NuSEDS dataset"))
}else{
  print(paste("There is the same number of time series (n =",nrow(timeseries_new),") the updated cleaned NuSEDS dataset"))
}

```

**Check the locations:**
  
```{r, echo=FALSE}
cond_GFE_ID_new <- ! timeseries_new$GFE_ID %in% timeseries_old$GFE_ID

if(any(cond_GFE_ID_new)){
  
  GFE_ID_new <- timeseries_new$GFE_ID[cond_GFE_ID_new]
  cond <- nuseds$GFE_ID %in% GFE_ID_new
  
  print(paste0("There are ",sum(cond_GFE_ID_new)," new locations in the updated NuSEDS:"))
  show <- unique(nuseds[cond,c("region","sys_nm","GFE_ID","Y_LAT","X_LONGT")])
  rownames(show) <- NULL
  show <- show[order(show$region),]
  print(show)
  
}else{
  print("There is no new location in the updated NuSEDS")
}
```

```{r, echo=FALSE}
cond_GFE_ID_old <- ! timeseries_old$GFE_ID %in% timeseries_new$GFE_ID

if(any(cond_GFE_ID_old)){
  
  GFE_ID_old <- timeseries_old$GFE_ID[cond_GFE_ID_old]
  cond <- nuseds_old$GFE_ID %in% GFE_ID_old
  
  print(paste0("There are ",sum(cond_GFE_ID_old)," locations in the previous cleaned NuSEDS not in the updated one:"))
  show <- unique(nuseds[cond,c("region","sys_nm","GFE_ID","Y_LAT","X_LONGT")])
  rownames(show) <- NULL
  show <- show[order(show$region),]
  print(show)
  
}else{
  print("There is no location in the previous cleaned NuSEDS that is not in the updated one")
}
```

**Check the populations:**
  
```{r, echo=FALSE}
cond_IndexId_new <- ! timeseries_new$IndexId %in% timeseries_old$IndexId

if(any(cond_IndexId_new)){
  
  IndexId_new <- timeseries_new$IndexId[cond_IndexId_new]
  cond <- nuseds$IndexId %in% IndexId_new
  
  print(paste0("There are ",sum(cond_IndexId_new)," new IndexId (POP_ID) in the updated NuSEDS:"))
  show <- unique(nuseds[cond,c("region","IndexId","POPULATION","cuid","cu_name_pse")])
  rownames(show) <- NULL
  show <- show[order(show$region),]
  print(show)
  
}else{
  print("There is no new IndexId (POP_ID) in the updated NuSEDS")
}
```

```{r, echo=FALSE}
cond_IndexId_old <- ! timeseries_old$IndexId %in% timeseries_new$IndexId

if(any(cond_IndexId_old)){
  
  IndexId_old <- timeseries_old$IndexId[cond_IndexId_old]
  cond <- nuseds_old$IndexId %in% IndexId_old
  
  print(paste0("There are ",sum(cond_IndexId_old)," IndexId (POP_ID) in the previous cleaned NuSEDS not in the updated one:"))
  show <- unique(nuseds[cond,c("region","IndexId","POPULATION","cuid","cu_name_pse")])
  rownames(show) <- NULL
  show <- show[order(show$region),]
  print(show)
  
}else{
  print("There is no new IndexId (POP_ID) in the previous cleaned NuSEDS that is not in the updated one")
}
```

**Check the time series:**
  
```{r, echo=FALSE}
cond_new_only <- !timeseries_new$IndexId_GFE_ID %in% timeseries_old$IndexId_GFE_ID
timeseries_new_only <- timeseries_new$IndexId_GFE_ID[cond_new_only]

cond_old_only <- !timeseries_old$IndexId_GFE_ID %in% timeseries_new$IndexId_GFE_ID
timeseries_old_only <- timeseries_old$IndexId_GFE_ID[cond_old_only]
```

There are `r sum(cond_new_only)` time series only found in the updated NuSEDS:
  
```{r, echo=FALSE, fig.height=8, fig.width=12}
if(any(cond_new_only)){
  for(r in 1:nrow(timeseries_new[cond_new_only,])){
    # r <- 1
    cuid <- timeseries_new[cond_new_only,]$cuid[r]
    GFE_ID <- timeseries_new[cond_new_only,]$GFE_ID[r]
    IndexId <- timeseries_new[cond_new_only,]$IndexId[r]
    
    cond <- nuseds$IndexId == IndexId 
    region <- unique(nuseds$region[cond])
    cu_name_pse <- unique(nuseds$cu_name_pse[cond])
    
    plot_IndexId_GFE_ID_fun(IndexIds = IndexId,
                            GFE_IDs = GFE_ID,
                            all_areas_nuseds = nuseds, 
                            main = paste(region," - ",cu_name_pse," (",cuid,")",sep=""))
    
    cond_IndexId <- IndexId == nuseds_old$IndexId
    cond_GFE_ID <- GFE_ID == nuseds_old$GFE_ID
    
    
    if(!any(cond_IndexId) & any(cond_GFE_ID)){
      m <- "IndexId not in old cleaned NuSEDS"
      
    }else if(any(cond_IndexId) & !any(cond_GFE_ID)){
      m <- "GFE_ID not in old cleaned NuSEDS"
      
    }else if(!any(cond_IndexId) & !any(cond_GFE_ID)){
      m <- "IndexId and GFE_ID are not in old cleaned NuSEDS"
      
    }else if(!any(cond_IndexId & cond_GFE_ID)){
      m <- "The IndexId & GFE_ID is not in old cleaned NuSEDS"
      
    }else{
      m <- "???"
    }
    
    legend("topright",m,bty = "n")
    legend("bottomleft",paste0("r = ",r),bty = "n")
    
    # find the reason
    cond <- series_added_new$IndexId == IndexId & series_added_new$GFE_ID == GFE_ID
    if(any(cond)){
      print(paste0("For series ",IndexId," - ",GFE_ID,": ",series_added_new$comment[cond]))
      
    }else{
      cond <- series_removed_old$IndexId == IndexId & series_removed_old$GFE_ID == GFE_ID
      
      if(any(cond)){
        print(paste0("For series ",IndexId," - ",GFE_ID,": it was previously:",series_removed_old$comment[cond]))
      }else{
        print("NOT SURE WHY IT APPEARS NOW AND NOT PREVIOUSLY")
      }
    }
  }
}

```

There are `r sum(cond_old_only)` time series only found in the previous NuSEDS:
  
```{r, echo=FALSE, fig.height=8, fig.width=12}

if(any(cond_old_only)){
  for(r in 1:nrow(timeseries_old[cond_old_only,])){
    # r <- 4
    cuid <- timeseries_old[cond_old_only,]$cuid[r]
    GFE_ID <- timeseries_old[cond_old_only,]$GFE_ID[r]
    IndexId <- timeseries_old[cond_old_only,]$IndexId[r]
    
    cond <- nuseds$IndexId == IndexId 
    region <- unique(nuseds$region[cond])
    cu_name_pse <- unique(nuseds$cu_name_pse[cond])
    
    plot_IndexId_GFE_ID_fun(IndexIds = IndexId,
                            GFE_IDs = GFE_ID,
                            all_areas_nuseds = nuseds, 
                            main = paste(region," - ",cu_name_pse," (",cuid,")",sep=""))
    
    cond_IndexId <- IndexId == nuseds_new$IndexId
    cond_GFE_ID <- GFE_ID == nuseds_new$GFE_ID
    
    if(!any(cond_IndexId) & any(cond_GFE_ID)){
      m <- "IndexId not in old cleaned NuSEDS"
      
    }else if(any(cond_IndexId) & !any(cond_GFE_ID)){
      m <- "GFE_ID not in old cleaned NuSEDS"
      
    }else if(!any(cond_IndexId) & !any(cond_GFE_ID)){
      m <- "IndexId and GFE_ID are not in old cleaned NuSEDS"
      
    }else if(!any(cond_IndexId & cond_GFE_ID)){
      m <- "The IndexId & GFE_ID is not in old cleaned NuSEDS"
      
    }else{
      m <- "???"
    }
    
    legend("topright",m,bty = "n")
    legend("bottomleft",paste0("r = ",r),bty = "n")
  }
}

```







